name: Data Scraping Automation

on:
  # Run manually when triggered
  workflow_dispatch:
  
  # Optionally run on a schedule
  # schedule:
  #   - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
permissions:
  contents: write  # To push changes back to the repo
  actions: read    # To upload artifacts
jobs:
  scrape-data:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hour timeout
    
    env:
      TERM: xterm  # Prevent TERM environment variable warnings

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Install Playwright and browsers
        pip install playwright
        playwright install --with-deps
    
    - name: Verify input files
      run: |
        if [ ! -f "urls with titles.csv" ]; then
          echo "Error: urls with titles.csv not found"
          exit 1
        fi
        echo "Input file found, starting scraper..."
    
    - name: Run scraper with progress monitoring
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        YOUR_SITE_URL: ${{ secrets.YOUR_SITE_URL }}
        YOUR_SITE_NAME: ${{ secrets.YOUR_SITE_NAME }}
        CI: true
        GITHUB_ACTIONS: true
      timeout-minutes: 240  # 4 hour timeout for scraper step
      run: |
        # Start progress monitoring in background
        (
          while true; do
            sleep 300  # Every 5 minutes
            echo "=== PROGRESS CHECK $(date) ==="
            if [ -f "pricing_results_with_resume_checkpoint.json" ]; then
              echo "✅ Checkpoint file exists"
              ls -la *checkpoint* || true
            fi
            if [ -f "pricing_results_with_resume.json" ]; then
              echo "✅ Results file exists"
              ls -la *results* || true
            fi
            echo "=== Still running... ==="
          done
        ) &
        
        # Run main scraper with timeout
        timeout 18000 python main.py || echo "Scraper completed or timed out at $(date)"
        
        # Kill background progress monitor
        jobs -p | xargs -r kill || true

    - name: List all files for debugging
      if: always()
      run: |
        echo "=== LISTING ALL FILES ==="
        ls -la
        echo "=== CHECKING FOR RESULT FILES ==="
        ls -la *results* || echo "No result files found"
        ls -la *checkpoint* || echo "No checkpoint files found"

    # ✅ Upload intermediate results immediately after scraper (regardless of success/failure)
    - name: Upload intermediate results
      uses: actions/upload-artifact@v4
      if: always()  # Always upload, even if scraper fails or times out
      with:
        name: intermediate-results-${{ github.run_number }}
        path: |
          pricing_results_with_resume.json
          pricing_results_with_resume_checkpoint.json
        if-no-files-found: warn  # Don't fail if no files found
    
    - name: Check output files
      run: |
        if [ -f "pricing_results_with_resume.json" ]; then
          echo "✅ Main results file created"
          wc -l "pricing_results_with_resume.json"
        else
          echo "❌ Main results file not found"
        fi
        
        if [ -f "pricing_results_with_resume_checkpoint.json" ]; then
          echo "✅ Checkpoint file found"
        else
          echo "ℹ️ No checkpoint file (normal if completed)"
        fi
    
    - name: Upload results as artifact
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if previous steps failed
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          pricing_results_with_resume.json
          pricing_results_with_resume_checkpoint.json
        retention-days: 30
    
    - name: Commit and push results
      if: success()  # Only commit if scraping succeeded
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        if ! git diff --cached --quiet; then
          git commit -m "Update scraping results - $(date '+%Y-%m-%d %H:%M:%S UTC')"
          git push
        else
          echo "No changes to commit"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
