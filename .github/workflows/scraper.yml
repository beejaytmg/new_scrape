name: Data Scraping Automation

on:
  # Run manually when triggered
  workflow_dispatch:
  
  # Optionally run on a schedule
  # schedule:
  #   - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    timeout-minutes: 340  # 8 hour timeout
    
    env:
      TERM: xterm  # Prevent TERM environment variable warnings

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Install Playwright and browsers
        pip install playwright
        playwright install --with-deps
    
    - name: Verify input files
      run: |
        if [ ! -f "urls with titles.csv" ]; then
          echo "Error: urls with titles.csv not found"
          exit 1
        fi
        echo "Input file found, starting scraper..."
    
    - name: Run scraper
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        YOUR_SITE_URL: ${{ secrets.YOUR_SITE_URL }}
        YOUR_SITE_NAME: ${{ secrets.YOUR_SITE_NAME }}
      run: |
        python main.py

    # ✅ NEW STEP: Upload partial results right after scraper
    - name: Upload partial results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: partial-results-${{ github.run_number }}
        path: |
          pricing_results_with_resume.json
          pricing_results_with_resume_checkpoint.json
    
    - name: Check output files
      run: |
        if [ -f "pricing_results_with_resume.json" ]; then
          echo "✅ Main results file created"
          wc -l "pricing_results_with_resume.json"
        else
          echo "❌ Main results file not found"
        fi
        
        if [ -f "pricing_results_with_resume_checkpoint.json" ]; then
          echo "✅ Checkpoint file found"
        else
          echo "ℹ️ No checkpoint file (normal if completed)"
        fi
    
    - name: Upload results as artifact
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if previous steps failed
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          pricing_results_with_resume.json
          pricing_results_with_resume_checkpoint.json
        retention-days: 30
    
    - name: Commit and push results
      if: success()  # Only commit if scraping succeeded
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        if ! git diff --cached --quiet; then
          git commit -m "Update scraping results - $(date '+%Y-%m-%d %H:%M:%S UTC')"
          git push
        else
          echo "No changes to commit"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
